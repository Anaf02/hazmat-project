{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image generation using GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory growth for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images and augment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to the dataset used and setting the image size and batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 64, 64\n",
    "batch_size = 32\n",
    "train_datagen  = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rescale=1.0 / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    "    data_format=\"channels_last\",\n",
    "    brightness_range=[0.5, 1.5],\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 images belonging to 13 classes.\n",
      "Found 13 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"./../data/images/classes\"  # Enter Directory of all images\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"./../data/images/classes\",          # Directory path to the training dataset\n",
    "    target_size=(img_height, img_width),   # Resize images to 150x150 pixels\n",
    "    batch_size=32,            # Number of images to yield per batch\n",
    "    class_mode='categorical'  # Use 'categorical' for multi-class classification\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    \"./../data/images/classes\",     # Directory path to the validation dataset\n",
    "    target_size=(img_height, img_width),   # Resize images to 150x150 pixels\n",
    "    batch_size=32,            # Number of images to yield per batch\n",
    "    class_mode='categorical'  # Use 'categorical' for multi-class classification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data augmentation on image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\hazmat-env\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\Ana\\miniconda3\\envs\\hazmat-env\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step - accuracy: 0.0000e+00 - loss: 2.5607 - val_accuracy: 0.0769 - val_loss: 2.5383\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.0769 - loss: 2.5523 - val_accuracy: 0.0769 - val_loss: 2.5101\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.0769 - loss: 2.5271 - val_accuracy: 0.2308 - val_loss: 2.4523\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.1538 - loss: 2.4731 - val_accuracy: 0.1538 - val_loss: 2.3584\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.2308 - loss: 2.4196 - val_accuracy: 0.1538 - val_loss: 2.2475\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.2308 - loss: 2.2898 - val_accuracy: 0.3077 - val_loss: 2.1495\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.3077 - loss: 2.1563 - val_accuracy: 0.3077 - val_loss: 1.9608\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.2308 - loss: 2.0503 - val_accuracy: 0.3846 - val_loss: 1.7474\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 0.3077 - loss: 1.9069 - val_accuracy: 0.3077 - val_loss: 1.7200\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.2308 - loss: 1.8095 - val_accuracy: 0.5385 - val_loss: 1.3922\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.6154 - loss: 1.6431 - val_accuracy: 0.6923 - val_loss: 1.1522\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.6154 - loss: 1.3388 - val_accuracy: 0.7692 - val_loss: 1.1693\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.6923 - loss: 1.1484 - val_accuracy: 0.6923 - val_loss: 0.8920\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.6154 - loss: 1.0778 - val_accuracy: 0.6154 - val_loss: 0.9986\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.6923 - loss: 0.9391 - val_accuracy: 0.6923 - val_loss: 0.7867\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.6923 - loss: 1.0286 - val_accuracy: 0.6154 - val_loss: 0.8346\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.2308 - loss: 1.4848 - val_accuracy: 0.4615 - val_loss: 1.4363\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.6923 - loss: 0.9377 - val_accuracy: 0.3846 - val_loss: 2.0456\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.5385 - loss: 1.2954 - val_accuracy: 0.7692 - val_loss: 0.7871\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.5385 - loss: 1.0111 - val_accuracy: 0.8462 - val_loss: 0.6130\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.5385 - loss: 1.2462 - val_accuracy: 0.6154 - val_loss: 0.7009\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.4615 - loss: 1.1657 - val_accuracy: 0.7692 - val_loss: 0.8042\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.7692 - loss: 0.7037 - val_accuracy: 0.5385 - val_loss: 1.3916\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.6154 - loss: 1.2692 - val_accuracy: 0.4615 - val_loss: 1.0600\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.5385 - loss: 0.8483 - val_accuracy: 0.6154 - val_loss: 0.7644\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.5385 - loss: 0.9644 - val_accuracy: 0.9231 - val_loss: 0.5047\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.7692 - loss: 0.6360 - val_accuracy: 0.6154 - val_loss: 0.7844\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.6923 - loss: 1.2922 - val_accuracy: 0.6923 - val_loss: 0.6697\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.7692 - loss: 0.6243 - val_accuracy: 0.7692 - val_loss: 0.5403\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.8462 - loss: 0.7807 - val_accuracy: 0.7692 - val_loss: 0.4950\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.7692 - loss: 0.6417 - val_accuracy: 0.8462 - val_loss: 0.4870\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.7692 - loss: 0.6071 - val_accuracy: 0.6923 - val_loss: 0.6274\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.7692 - loss: 0.9734 - val_accuracy: 0.7692 - val_loss: 0.6417\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.7692 - loss: 0.7816 - val_accuracy: 0.6923 - val_loss: 0.6164\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8462 - loss: 0.6978 - val_accuracy: 0.7692 - val_loss: 0.5007\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.7692 - loss: 0.5566 - val_accuracy: 0.7692 - val_loss: 0.4151\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.6923 - loss: 0.4960 - val_accuracy: 0.8462 - val_loss: 0.3373\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.6154 - loss: 0.5724 - val_accuracy: 0.8462 - val_loss: 0.3101\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8462 - loss: 0.5580 - val_accuracy: 0.9231 - val_loss: 0.2948\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.7692 - loss: 0.5796 - val_accuracy: 0.7692 - val_loss: 0.5168\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.7692 - loss: 0.5966 - val_accuracy: 0.6923 - val_loss: 0.6683\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8462 - loss: 0.5827 - val_accuracy: 0.7692 - val_loss: 0.4661\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.6923 - loss: 0.7659 - val_accuracy: 0.8462 - val_loss: 0.3482\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.7692 - loss: 0.4529 - val_accuracy: 0.8462 - val_loss: 0.3733\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.5385 - loss: 0.8741 - val_accuracy: 0.8462 - val_loss: 0.6279\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.6923 - loss: 0.7623 - val_accuracy: 0.7692 - val_loss: 0.9673\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8462 - loss: 0.4689 - val_accuracy: 0.6154 - val_loss: 1.4603\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.6923 - loss: 1.3450 - val_accuracy: 0.7692 - val_loss: 0.8271\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.9231 - loss: 0.3205 - val_accuracy: 0.8462 - val_loss: 0.6093\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.4615 - loss: 1.1455 - val_accuracy: 0.7692 - val_loss: 0.4292\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.6923 - loss: 0.6858 - val_accuracy: 0.9231 - val_loss: 0.2289\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.7692 - loss: 0.5402 - val_accuracy: 0.8462 - val_loss: 0.3168\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.7692 - loss: 0.5793 - val_accuracy: 0.7692 - val_loss: 0.5734\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.6154 - loss: 1.1792 - val_accuracy: 0.8462 - val_loss: 0.8710\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8462 - loss: 0.4784 - val_accuracy: 0.7692 - val_loss: 1.0915\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.7692 - loss: 0.6635 - val_accuracy: 0.6923 - val_loss: 1.0527\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.7692 - loss: 0.5596 - val_accuracy: 0.7692 - val_loss: 0.8231\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.6923 - loss: 0.6581 - val_accuracy: 0.8462 - val_loss: 0.5606\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8462 - loss: 0.3874 - val_accuracy: 0.7692 - val_loss: 0.3894\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8462 - loss: 0.4708 - val_accuracy: 0.8462 - val_loss: 0.3106\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.7692 - loss: 0.6658 - val_accuracy: 0.9231 - val_loss: 0.2622\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.7692 - loss: 0.6517 - val_accuracy: 0.9231 - val_loss: 0.2857\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8462 - loss: 0.5110 - val_accuracy: 0.7692 - val_loss: 0.4786\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.9231 - loss: 0.3458 - val_accuracy: 0.6923 - val_loss: 0.7943\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.9231 - loss: 0.3593 - val_accuracy: 0.6154 - val_loss: 0.9783\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 0.4779 - val_accuracy: 0.6154 - val_loss: 0.7960\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.7692 - loss: 0.4078 - val_accuracy: 0.8462 - val_loss: 0.4975\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.8462 - loss: 0.3968 - val_accuracy: 0.8462 - val_loss: 0.3066\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.2553 - val_accuracy: 0.9231 - val_loss: 0.3344\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.8462 - loss: 0.2974 - val_accuracy: 0.8462 - val_loss: 0.3935\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.8462 - loss: 0.2887 - val_accuracy: 0.8462 - val_loss: 0.4083\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9231 - loss: 0.2896 - val_accuracy: 0.8462 - val_loss: 0.6773\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 1.0000 - loss: 0.2033 - val_accuracy: 0.7692 - val_loss: 1.1198\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - accuracy: 0.8462 - loss: 0.2789 - val_accuracy: 0.7692 - val_loss: 1.3190\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.7692 - loss: 0.3958 - val_accuracy: 0.7692 - val_loss: 0.9216\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - accuracy: 0.8462 - loss: 0.2633 - val_accuracy: 0.8462 - val_loss: 0.4942\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8462 - loss: 0.2479 - val_accuracy: 0.9231 - val_loss: 0.2496\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 1.0000 - loss: 0.2301 - val_accuracy: 0.9231 - val_loss: 0.1433\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.9231 - loss: 0.1495 - val_accuracy: 0.9231 - val_loss: 0.1208\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.7692 - loss: 0.4541 - val_accuracy: 0.8462 - val_loss: 0.2454\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9231 - loss: 0.1782 - val_accuracy: 0.9231 - val_loss: 0.5299\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 0.1231 - val_accuracy: 0.7692 - val_loss: 0.9608\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.7692 - loss: 0.6184 - val_accuracy: 0.8462 - val_loss: 0.9521\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9231 - loss: 0.1832 - val_accuracy: 0.6923 - val_loss: 1.1237\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8462 - loss: 0.3674 - val_accuracy: 0.8462 - val_loss: 0.7060\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9231 - loss: 0.5014 - val_accuracy: 0.8462 - val_loss: 0.3507\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.1282 - val_accuracy: 0.8462 - val_loss: 0.2231\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9231 - loss: 0.1660 - val_accuracy: 0.9231 - val_loss: 0.1578\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 0.1479 - val_accuracy: 0.9231 - val_loss: 0.3265\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.1620 - val_accuracy: 0.8462 - val_loss: 0.4497\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.8462 - loss: 0.4595 - val_accuracy: 0.9231 - val_loss: 0.2570\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.8462 - loss: 0.6119 - val_accuracy: 0.7692 - val_loss: 0.5483\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 1.0000 - loss: 0.1424 - val_accuracy: 0.8462 - val_loss: 0.9562\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.9231 - loss: 0.3032 - val_accuracy: 0.8462 - val_loss: 1.2135\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.8462 - loss: 0.4943 - val_accuracy: 0.6923 - val_loss: 1.9664\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.9231 - loss: 0.2655 - val_accuracy: 0.6923 - val_loss: 1.4797\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.8462 - loss: 0.3762 - val_accuracy: 0.9231 - val_loss: 0.7517\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.9231 - loss: 0.1671 - val_accuracy: 0.8462 - val_loss: 0.6609\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9231 - loss: 0.3778 - val_accuracy: 1.0000 - val_loss: 0.1449\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.8462 - loss: 0.4628 - val_accuracy: 1.0000 - val_loss: 0.1259\n"
     ]
    }
   ],
   "source": [
    "simple_classifier = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32, (3, 3), activation=\"relu\", input_shape=(img_height, img_width, 3)\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(\n",
    "            num_classes, activation=\"softmax\"\n",
    "        ),  # 13 output neurons for 13 classes\n",
    "    ]\n",
    ")\n",
    "simple_classifier.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = simple_classifier.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=100,  # Number of epochs to train\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "\n",
    "save_model(simple_classifier, './../models/simple_classifier.keras')  # Save generator in native Keras format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example image path for prediction\n",
    "img_path = './../data/images/classes/dangerous/6-dangerous.png'\n",
    "\n",
    "# Load and preprocess the image\n",
    "img = load_img(img_path, target_size=(img_height, img_width))\n",
    "img_array = img_to_array(img)\n",
    "img_array = img_array / 255.0  # Rescale to [0, 1]\n",
    "img_array = tf.expand_dims(img_array, 0)  # Expand dimensions to create batch size of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = simple_classifier.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_index = np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: dangerous\n"
     ]
    }
   ],
   "source": [
    "class_labels = list(train_generator.class_indices.keys())  # Assuming train_generator is used for training\n",
    "predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "print(\"Predicted class:\", predicted_class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2DTranspose, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim):\n",
    "    model =Sequential()\n",
    "    \n",
    "   # Initial Dense layer\n",
    "    model.add(Dense(256 * 8 * 8, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Reshape((8, 8, 256)))\n",
    "    \n",
    "    # Upsampling to 16x16\n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    \n",
    "    # Upsampling to 32x32\n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    \n",
    "    # Upsampling to 64x64\n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    \n",
    "    # Final convolutional layer to generate 3 channels (RGB)\n",
    "    model.add(Conv2D(3, kernel_size=3, activation='tanh', padding='same'))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\miniconda3\\envs\\hazmat-env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = build_generator(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16384</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,113,536</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16384</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,416</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,459</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16384\u001b[0m)          │     \u001b[38;5;34m2,113,536\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16384\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m524,416\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m262,272\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m262,272\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │         \u001b[38;5;34m3,459\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,165,955</span> (12.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,165,955\u001b[0m (12.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,165,955</span> (12.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,165,955\u001b[0m (12.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9999789..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9999734..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9999288..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9999887..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a batch of random noise vectors\n",
    "noise = np.random.randn(4, latent_dim)  # 4 is the batch size\n",
    "\n",
    "# Generate images from the noise vectors\n",
    "generated_images = generator.predict(noise)\n",
    "\n",
    "# Inspect the generated images\n",
    "print(generated_images.shape)  # Should be (4, 200, 200, 3)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "\n",
    "# Display each generated image in a subplot\n",
    "for idx, img in enumerate(generated_images):\n",
    "    ax[idx].imshow(np.squeeze(img))\n",
    "    ax[idx].title.set_text(f\"Image {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Convolutional layers with Leaky ReLU activation\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=img_shape, padding='same'))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(512, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Flatten and classify\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "img_shape=(img_height, img_width,3)\n",
    "discriminator = build_discriminator(img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_100\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_100\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ real_or_fake (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,193</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_12 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_45 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_40 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_36 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_46 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_41 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_37 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_47 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_42 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_38 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_48 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_43 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_39 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ real_or_fake (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m8,193\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,559,169</span> (5.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,559,169\u001b[0m (5.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,559,169</span> (5.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,559,169\u001b[0m (5.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "(64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "generated_image = generator.predict(noise)[0]\n",
    "\n",
    "print(generated_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.47591844]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confidence of discriminator before training: obviously not too confident ~49%\n",
    "discriminator.predict(np.expand_dims(generated_image, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "discriminator.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 images belonging to 13 classes.\n",
      "Epoch 1/100\n",
      "Step 1/1 - Discriminator Loss: 1.1510306596755981, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[0.5682137 ]\n",
      " [0.58326805]\n",
      " [0.5678202 ]\n",
      " [0.5870667 ]\n",
      " [0.5776701 ]\n",
      " [0.5747747 ]\n",
      " [0.57682794]\n",
      " [0.581953  ]\n",
      " [0.57720065]\n",
      " [0.57481444]\n",
      " [0.5618424 ]\n",
      " [0.59020287]\n",
      " [0.57450455]]\n",
      "Epoch 2/100\n",
      "Step 1/1 - Discriminator Loss: -0.10218624025583267, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Predictions: [[0.6603046 ]\n",
      " [0.6802877 ]\n",
      " [0.6735172 ]\n",
      " [0.6571798 ]\n",
      " [0.64400476]\n",
      " [0.66315085]\n",
      " [0.65568686]\n",
      " [0.65423626]\n",
      " [0.6410616 ]\n",
      " [0.6441016 ]\n",
      " [0.6722163 ]\n",
      " [0.67860425]\n",
      " [0.6627501 ]]\n",
      "Epoch 3/100\n",
      "Step 1/1 - Discriminator Loss: -1.2885609865188599, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[0.7748484 ]\n",
      " [0.7479719 ]\n",
      " [0.73352224]\n",
      " [0.78348285]\n",
      " [0.76226497]\n",
      " [0.7773327 ]\n",
      " [0.75306916]\n",
      " [0.7570677 ]\n",
      " [0.7840768 ]\n",
      " [0.73468757]\n",
      " [0.7608315 ]\n",
      " [0.7344338 ]\n",
      " [0.7468684 ]]\n",
      "Epoch 4/100\n",
      "Step 1/1 - Discriminator Loss: -2.5579018592834473, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[0.85134995]\n",
      " [0.86942995]\n",
      " [0.83714074]\n",
      " [0.8395593 ]\n",
      " [0.86268234]\n",
      " [0.8594293 ]\n",
      " [0.8403229 ]\n",
      " [0.8819103 ]\n",
      " [0.8503287 ]\n",
      " [0.88499784]\n",
      " [0.86627585]\n",
      " [0.88841105]\n",
      " [0.89031404]]\n",
      "Epoch 5/100\n",
      "Step 1/1 - Discriminator Loss: -4.21683931350708, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Predictions: [[0.953479  ]\n",
      " [0.9344092 ]\n",
      " [0.934163  ]\n",
      " [0.94145834]\n",
      " [0.9635658 ]\n",
      " [0.94738513]\n",
      " [0.9618717 ]\n",
      " [0.96474546]\n",
      " [0.93147504]\n",
      " [0.9511459 ]\n",
      " [0.939804  ]\n",
      " [0.96625656]\n",
      " [0.948963  ]]\n",
      "Epoch 6/100\n",
      "Step 1/1 - Discriminator Loss: -6.198668003082275, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[0.99068755]\n",
      " [0.98778623]\n",
      " [0.99470717]\n",
      " [0.98396564]\n",
      " [0.98530364]\n",
      " [0.9951723 ]\n",
      " [0.99016416]\n",
      " [0.9898716 ]\n",
      " [0.9867913 ]\n",
      " [0.99408823]\n",
      " [0.9916298 ]\n",
      " [0.99462724]\n",
      " [0.98546284]]\n",
      "Epoch 7/100\n",
      "Step 1/1 - Discriminator Loss: -9.01136302947998, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[0.9994809 ]\n",
      " [0.99851906]\n",
      " [0.9987762 ]\n",
      " [0.99932426]\n",
      " [0.99873066]\n",
      " [0.99935764]\n",
      " [0.9997928 ]\n",
      " [0.9990489 ]\n",
      " [0.99971336]\n",
      " [0.9993048 ]\n",
      " [0.9988753 ]\n",
      " [0.9997555 ]\n",
      " [0.99976027]]\n",
      "Epoch 8/100\n",
      "Step 1/1 - Discriminator Loss: -12.992080688476562, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[0.9999908 ]\n",
      " [0.9999982 ]\n",
      " [0.9999671 ]\n",
      " [0.9999978 ]\n",
      " [0.9999775 ]\n",
      " [0.999975  ]\n",
      " [0.9999768 ]\n",
      " [0.9999939 ]\n",
      " [0.99998385]\n",
      " [0.99999046]\n",
      " [0.9999909 ]\n",
      " [0.9999987 ]\n",
      " [0.9999984 ]]\n",
      "Epoch 9/100\n",
      "Step 1/1 - Discriminator Loss: -18.790327072143555, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[0.9999999 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]]\n",
      "Epoch 10/100\n",
      "Step 1/1 - Discriminator Loss: -26.93480682373047, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 11/100\n",
      "Step 1/1 - Discriminator Loss: -38.56144332885742, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 12/100\n",
      "Step 1/1 - Discriminator Loss: -55.23614501953125, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 13/100\n",
      "Step 1/1 - Discriminator Loss: -80.10067749023438, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 14/100\n",
      "Step 1/1 - Discriminator Loss: -114.88951873779297, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 15/100\n",
      "Step 1/1 - Discriminator Loss: -163.5183868408203, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 16/100\n",
      "Step 1/1 - Discriminator Loss: -231.64764404296875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 17/100\n",
      "Step 1/1 - Discriminator Loss: -324.44866943359375, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 18/100\n",
      "Step 1/1 - Discriminator Loss: -450.2950744628906, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 19/100\n",
      "Step 1/1 - Discriminator Loss: -620.8703002929688, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 20/100\n",
      "Step 1/1 - Discriminator Loss: -851.3189086914062, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 21/100\n",
      "Step 1/1 - Discriminator Loss: -1147.4952392578125, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 22/100\n",
      "Step 1/1 - Discriminator Loss: -1534.6119384765625, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 23/100\n",
      "Step 1/1 - Discriminator Loss: -2020.641357421875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 24/100\n",
      "Step 1/1 - Discriminator Loss: -2647.81005859375, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 25/100\n",
      "Step 1/1 - Discriminator Loss: -3437.22607421875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 26/100\n",
      "Step 1/1 - Discriminator Loss: -4420.95947265625, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 27/100\n",
      "Step 1/1 - Discriminator Loss: -5608.48095703125, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 28/100\n",
      "Step 1/1 - Discriminator Loss: -7099.091796875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 29/100\n",
      "Step 1/1 - Discriminator Loss: -8883.662109375, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 30/100\n",
      "Step 1/1 - Discriminator Loss: -11077.35546875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 31/100\n",
      "Step 1/1 - Discriminator Loss: -13667.228515625, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 32/100\n",
      "Step 1/1 - Discriminator Loss: -16678.34765625, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 33/100\n",
      "Step 1/1 - Discriminator Loss: -20335.79296875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 34/100\n",
      "Step 1/1 - Discriminator Loss: -24619.158203125, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 35/100\n",
      "Step 1/1 - Discriminator Loss: -29628.435546875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 36/100\n",
      "Step 1/1 - Discriminator Loss: -35542.55078125, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 37/100\n",
      "Step 1/1 - Discriminator Loss: -42440.6015625, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 38/100\n",
      "Step 1/1 - Discriminator Loss: -50318.44140625, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 39/100\n",
      "Step 1/1 - Discriminator Loss: -59286.8359375, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 40/100\n",
      "Step 1/1 - Discriminator Loss: -69563.3671875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 41/100\n",
      "Step 1/1 - Discriminator Loss: -81389.734375, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 42/100\n",
      "Step 1/1 - Discriminator Loss: -94684.0625, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 43/100\n",
      "Step 1/1 - Discriminator Loss: -109874.3984375, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 44/100\n",
      "Step 1/1 - Discriminator Loss: -127125.2890625, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 45/100\n",
      "Step 1/1 - Discriminator Loss: -146458.96875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 46/100\n",
      "Step 1/1 - Discriminator Loss: -168249.671875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 47/100\n",
      "Step 1/1 - Discriminator Loss: -192490.375, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 48/100\n",
      "Step 1/1 - Discriminator Loss: -219367.125, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 49/100\n",
      "Step 1/1 - Discriminator Loss: -249732.5, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 50/100\n",
      "Step 1/1 - Discriminator Loss: -283065.65625, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 51/100\n",
      "Step 1/1 - Discriminator Loss: -319755.5, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 52/100\n",
      "Step 1/1 - Discriminator Loss: -360633.03125, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 53/100\n",
      "Step 1/1 - Discriminator Loss: -405842.625, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 54/100\n",
      "Step 1/1 - Discriminator Loss: -455560.625, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 55/100\n",
      "Step 1/1 - Discriminator Loss: -509746.84375, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 56/100\n",
      "Step 1/1 - Discriminator Loss: -569241.25, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 57/100\n",
      "Step 1/1 - Discriminator Loss: -634492.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 58/100\n",
      "Step 1/1 - Discriminator Loss: -706249.3125, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 59/100\n",
      "Step 1/1 - Discriminator Loss: -784136.9375, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 60/100\n",
      "Step 1/1 - Discriminator Loss: -868659.125, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 61/100\n",
      "Step 1/1 - Discriminator Loss: -959901.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 62/100\n",
      "Step 1/1 - Discriminator Loss: -1060748.875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 63/100\n",
      "Step 1/1 - Discriminator Loss: -1169139.25, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 64/100\n",
      "Step 1/1 - Discriminator Loss: -1286002.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 65/100\n",
      "Step 1/1 - Discriminator Loss: -1409856.125, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 66/100\n",
      "Step 1/1 - Discriminator Loss: -1544249.75, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 67/100\n",
      "Step 1/1 - Discriminator Loss: -1690382.75, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 68/100\n",
      "Step 1/1 - Discriminator Loss: -1847045.875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 69/100\n",
      "Step 1/1 - Discriminator Loss: -2014216.875, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 70/100\n",
      "Step 1/1 - Discriminator Loss: -2194027.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 71/100\n",
      "Step 1/1 - Discriminator Loss: -2386680.25, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 72/100\n",
      "Step 1/1 - Discriminator Loss: -2592494.5, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 73/100\n",
      "Step 1/1 - Discriminator Loss: -2815683.25, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 74/100\n",
      "Step 1/1 - Discriminator Loss: -3051643.25, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 75/100\n",
      "Step 1/1 - Discriminator Loss: -3298379.5, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 76/100\n",
      "Step 1/1 - Discriminator Loss: -3567371.5, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 77/100\n",
      "Step 1/1 - Discriminator Loss: -3851050.5, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 78/100\n",
      "Step 1/1 - Discriminator Loss: -4155047.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 79/100\n",
      "Step 1/1 - Discriminator Loss: -4479469.5, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 80/100\n",
      "Step 1/1 - Discriminator Loss: -4822654.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 81/100\n",
      "Step 1/1 - Discriminator Loss: -5184685.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 82/100\n",
      "Step 1/1 - Discriminator Loss: -5570764.5, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 83/100\n",
      "Step 1/1 - Discriminator Loss: -5978258.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 84/100\n",
      "Step 1/1 - Discriminator Loss: -6407694.5, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 85/100\n",
      "Step 1/1 - Discriminator Loss: -6862920.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 86/100\n",
      "Step 1/1 - Discriminator Loss: -7341505.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 87/100\n",
      "Step 1/1 - Discriminator Loss: -7852721.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 88/100\n",
      "Step 1/1 - Discriminator Loss: -8387809.5, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 89/100\n",
      "Step 1/1 - Discriminator Loss: -8948890.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 90/100\n",
      "Step 1/1 - Discriminator Loss: -9548491.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 91/100\n",
      "Step 1/1 - Discriminator Loss: -10173422.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 92/100\n",
      "Step 1/1 - Discriminator Loss: -10839074.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 93/100\n",
      "Step 1/1 - Discriminator Loss: -11528097.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 94/100\n",
      "Step 1/1 - Discriminator Loss: -12252374.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 95/100\n",
      "Step 1/1 - Discriminator Loss: -13020998.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 96/100\n",
      "Step 1/1 - Discriminator Loss: -13815367.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 97/100\n",
      "Step 1/1 - Discriminator Loss: -14667139.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 98/100\n",
      "Step 1/1 - Discriminator Loss: -15547183.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 99/100\n",
      "Step 1/1 - Discriminator Loss: -16470913.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Epoch 100/100\n",
      "Step 1/1 - Discriminator Loss: -17443484.0, Accuracy: 0.07692307978868484\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predictions: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Create training image generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    img_dir,                  # Directory path to the training dataset\n",
    "    target_size=(img_height, img_width),   # Resize images to 64x64 pixels\n",
    "    batch_size=batch_size,            # Number of images to yield per batch\n",
    "    class_mode='binary',          # No labels are required since we are training on real images only\n",
    "    shuffle=True              # Shuffle data\n",
    ")\n",
    "\n",
    "#Build and compile discriminator\n",
    "img_shape = (img_height, img_width, 3)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "discriminator.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the discriminator\n",
    "epochs = 100\n",
    "steps_per_epoch = len(train_generator)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for step in range(steps_per_epoch):\n",
    "        imgs, labels = next(train_generator)\n",
    "        d_loss = discriminator.train_on_batch(imgs, labels)\n",
    "        print(f\"Step {step+1}/{steps_per_epoch} - Discriminator Loss: {d_loss[0]}, Accuracy: {d_loss[1]}\")\n",
    "    \n",
    "    # Print predictions after every epoch for inspection\n",
    "    predictions = discriminator.predict(train_generator)\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizers and losses\n",
    "g_opt = Adam(learning_rate=0.0001) \n",
    "d_opt = Adam(learning_rate=0.00001) \n",
    "g_loss = BinaryCrossentropy()\n",
    "d_loss = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclassed Model for GAN\n",
    "class TrainGAN(Model):\n",
    "    def __init__(self, generator, discriminator, g_opt, d_opt, g_loss, d_loss, latent_dim=128, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        real_images = batch  # Extract images (no labels)\n",
    "        #batch_size = tf.shape(real_images)[0]\n",
    "        random_noise = tf.random.normal((128, self.latent_dim), mean=0, stddev=100)\n",
    "\n",
    "        # Generate fake images\n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "            gen_images = self.generator(random_noise, training=True)\n",
    "            yhat_real = self.discriminator(real_images, training=True)\n",
    "            yhat_fake = self.discriminator(gen_images, training=True)\n",
    "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
    "            y_realfake = tf.concat([tf.ones_like(yhat_real), tf.zeros_like(yhat_fake)], axis=0)\n",
    "\n",
    "            # Adding noise to labels for discriminator\n",
    "            noise_real = 0.15 * tf.random.uniform(tf.shape(yhat_real))\n",
    "            noise_fake = -0.15 * tf.random.uniform(tf.shape(yhat_fake))\n",
    "            y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n",
    "\n",
    "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
    "\n",
    "            predicted_labels = self.discriminator(gen_images, training=False)\n",
    "            total_g_loss = self.g_loss(tf.ones_like(predicted_labels), predicted_labels)\n",
    "\n",
    "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
    "\n",
    "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
    "\n",
    "        return {\"d_loss\": total_d_loss, \"g_loss\": total_g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback to save generated images during training\n",
    "class ModelMonitor(Callback):\n",
    "    def __init__(self, generator, num_img=3, latent_dim=128, save_dir='./../data/images/generated', *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.generator = generator\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "        self.save_dir = save_dir\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal((self.num_img, self.latent_dim))\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "        generated_images = (generated_images * 127.5 + 127.5).numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = array_to_img(generated_images[i])\n",
    "            img.save(os.path.join(self.save_dir, f'generated_img_{epoch}_{i}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GAN model instance\n",
    "gan = TrainGAN(generator, discriminator, g_opt, d_opt, g_loss, d_loss)\n",
    "gan.compile(optimizer=g_opt, loss=g_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 images belonging to 13 classes.\n",
      "Epoch 1/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4121 - g_loss: 0.0000e+00\n",
      "Epoch 2/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - d_loss: 15.4803 - g_loss: 0.0000e+00\n",
      "Epoch 3/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4364 - g_loss: 0.0000e+00\n",
      "Epoch 4/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5565 - g_loss: 0.0000e+00\n",
      "Epoch 5/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - d_loss: 15.4840 - g_loss: 0.0000e+00\n",
      "Epoch 6/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - d_loss: 15.5655 - g_loss: 0.0000e+00\n",
      "Epoch 7/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4021 - g_loss: 0.0000e+00\n",
      "Epoch 8/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - d_loss: 15.5574 - g_loss: 0.0000e+00\n",
      "Epoch 9/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4604 - g_loss: 0.0000e+00\n",
      "Epoch 10/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - d_loss: 15.4277 - g_loss: 0.0000e+00\n",
      "Epoch 11/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4308 - g_loss: 0.0000e+00\n",
      "Epoch 12/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4497 - g_loss: 0.0000e+00\n",
      "Epoch 13/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4257 - g_loss: 0.0000e+00\n",
      "Epoch 14/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5602 - g_loss: 0.0000e+00\n",
      "Epoch 15/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4576 - g_loss: 0.0000e+00\n",
      "Epoch 16/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5042 - g_loss: 0.0000e+00\n",
      "Epoch 17/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4149 - g_loss: 0.0000e+00\n",
      "Epoch 18/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4512 - g_loss: 0.0000e+00\n",
      "Epoch 19/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3737 - g_loss: 0.0000e+00\n",
      "Epoch 20/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4594 - g_loss: 0.0000e+00\n",
      "Epoch 21/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4081 - g_loss: 0.0000e+00\n",
      "Epoch 22/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3965 - g_loss: 0.0000e+00\n",
      "Epoch 23/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4304 - g_loss: 0.0000e+00\n",
      "Epoch 24/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4931 - g_loss: 0.0000e+00\n",
      "Epoch 25/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4884 - g_loss: 0.0000e+00\n",
      "Epoch 26/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4328 - g_loss: 0.0000e+00\n",
      "Epoch 27/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4281 - g_loss: 0.0000e+00\n",
      "Epoch 28/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4124 - g_loss: 0.0000e+00\n",
      "Epoch 29/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4990 - g_loss: 0.0000e+00\n",
      "Epoch 30/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4632 - g_loss: 0.0000e+00\n",
      "Epoch 31/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4145 - g_loss: 0.0000e+00\n",
      "Epoch 32/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4961 - g_loss: 0.0000e+00\n",
      "Epoch 33/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4638 - g_loss: 0.0000e+00\n",
      "Epoch 34/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4200 - g_loss: 0.0000e+00\n",
      "Epoch 35/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4150 - g_loss: 0.0000e+00\n",
      "Epoch 36/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4666 - g_loss: 0.0000e+00\n",
      "Epoch 37/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4099 - g_loss: 0.0000e+00\n",
      "Epoch 38/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3365 - g_loss: 0.0000e+00\n",
      "Epoch 39/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3811 - g_loss: 0.0000e+00\n",
      "Epoch 40/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4722 - g_loss: 0.0000e+00\n",
      "Epoch 41/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3850 - g_loss: 0.0000e+00\n",
      "Epoch 42/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5126 - g_loss: 0.0000e+00\n",
      "Epoch 43/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4764 - g_loss: 0.0000e+00\n",
      "Epoch 44/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4458 - g_loss: 0.0000e+00\n",
      "Epoch 45/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4431 - g_loss: 0.0000e+00\n",
      "Epoch 46/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5250 - g_loss: 0.0000e+00\n",
      "Epoch 47/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4872 - g_loss: 0.0000e+00\n",
      "Epoch 48/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4016 - g_loss: 0.0000e+00\n",
      "Epoch 49/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5238 - g_loss: 0.0000e+00\n",
      "Epoch 50/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4541 - g_loss: 0.0000e+00\n",
      "Epoch 51/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4066 - g_loss: 0.0000e+00\n",
      "Epoch 52/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4276 - g_loss: 0.0000e+00\n",
      "Epoch 53/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4789 - g_loss: 0.0000e+00\n",
      "Epoch 54/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3687 - g_loss: 0.0000e+00\n",
      "Epoch 55/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3655 - g_loss: 0.0000e+00\n",
      "Epoch 56/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3683 - g_loss: 0.0000e+00\n",
      "Epoch 57/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4965 - g_loss: 0.0000e+00\n",
      "Epoch 58/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4778 - g_loss: 0.0000e+00\n",
      "Epoch 59/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3455 - g_loss: 0.0000e+00\n",
      "Epoch 60/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3701 - g_loss: 0.0000e+00\n",
      "Epoch 61/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4475 - g_loss: 0.0000e+00\n",
      "Epoch 62/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4317 - g_loss: 0.0000e+00\n",
      "Epoch 63/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4674 - g_loss: 0.0000e+00\n",
      "Epoch 64/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4022 - g_loss: 0.0000e+00\n",
      "Epoch 65/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4134 - g_loss: 0.0000e+00\n",
      "Epoch 66/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5040 - g_loss: 0.0000e+00\n",
      "Epoch 67/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4601 - g_loss: 0.0000e+00\n",
      "Epoch 68/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5414 - g_loss: 0.0000e+00\n",
      "Epoch 69/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3749 - g_loss: 0.0000e+00\n",
      "Epoch 70/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5356 - g_loss: 0.0000e+00\n",
      "Epoch 71/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4779 - g_loss: 0.0000e+00\n",
      "Epoch 72/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4973 - g_loss: 0.0000e+00\n",
      "Epoch 73/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4804 - g_loss: 0.0000e+00\n",
      "Epoch 74/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4153 - g_loss: 0.0000e+00\n",
      "Epoch 75/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4741 - g_loss: 0.0000e+00\n",
      "Epoch 76/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4703 - g_loss: 0.0000e+00\n",
      "Epoch 77/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5357 - g_loss: 0.0000e+00\n",
      "Epoch 78/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4887 - g_loss: 0.0000e+00\n",
      "Epoch 79/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4087 - g_loss: 0.0000e+00\n",
      "Epoch 80/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4324 - g_loss: 0.0000e+00\n",
      "Epoch 81/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4107 - g_loss: 0.0000e+00\n",
      "Epoch 82/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4621 - g_loss: 0.0000e+00\n",
      "Epoch 83/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4095 - g_loss: 0.0000e+00\n",
      "Epoch 84/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5504 - g_loss: 0.0000e+00\n",
      "Epoch 85/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5219 - g_loss: 0.0000e+00\n",
      "Epoch 86/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3797 - g_loss: 0.0000e+00\n",
      "Epoch 87/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4772 - g_loss: 0.0000e+00\n",
      "Epoch 88/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4070 - g_loss: 0.0000e+00\n",
      "Epoch 89/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4694 - g_loss: 0.0000e+00\n",
      "Epoch 90/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4773 - g_loss: 0.0000e+00\n",
      "Epoch 91/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5655 - g_loss: 0.0000e+00\n",
      "Epoch 92/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5364 - g_loss: 0.0000e+00\n",
      "Epoch 93/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5449 - g_loss: 0.0000e+00\n",
      "Epoch 94/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4770 - g_loss: 0.0000e+00\n",
      "Epoch 95/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4151 - g_loss: 0.0000e+00\n",
      "Epoch 96/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4395 - g_loss: 0.0000e+00\n",
      "Epoch 97/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4167 - g_loss: 0.0000e+00\n",
      "Epoch 98/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5695 - g_loss: 0.0000e+00\n",
      "Epoch 99/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4372 - g_loss: 0.0000e+00\n",
      "Epoch 100/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3252 - g_loss: 0.0000e+00\n",
      "Epoch 101/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5568 - g_loss: 0.0000e+00\n",
      "Epoch 102/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4788 - g_loss: 0.0000e+00\n",
      "Epoch 103/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4626 - g_loss: 0.0000e+00\n",
      "Epoch 104/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4848 - g_loss: 0.0000e+00\n",
      "Epoch 105/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4395 - g_loss: 0.0000e+00\n",
      "Epoch 106/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5467 - g_loss: 0.0000e+00\n",
      "Epoch 107/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3929 - g_loss: 0.0000e+00\n",
      "Epoch 108/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3927 - g_loss: 0.0000e+00\n",
      "Epoch 109/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4756 - g_loss: 0.0000e+00\n",
      "Epoch 110/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5519 - g_loss: 0.0000e+00\n",
      "Epoch 111/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4879 - g_loss: 0.0000e+00\n",
      "Epoch 112/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4416 - g_loss: 0.0000e+00\n",
      "Epoch 113/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4926 - g_loss: 0.0000e+00\n",
      "Epoch 114/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4801 - g_loss: 0.0000e+00\n",
      "Epoch 115/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4873 - g_loss: 0.0000e+00\n",
      "Epoch 116/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4181 - g_loss: 0.0000e+00\n",
      "Epoch 117/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4469 - g_loss: 0.0000e+00\n",
      "Epoch 118/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5049 - g_loss: 0.0000e+00\n",
      "Epoch 119/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4435 - g_loss: 0.0000e+00\n",
      "Epoch 120/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5256 - g_loss: 0.0000e+00\n",
      "Epoch 121/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4615 - g_loss: 0.0000e+00\n",
      "Epoch 122/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4192 - g_loss: 0.0000e+00\n",
      "Epoch 123/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5515 - g_loss: 0.0000e+00\n",
      "Epoch 124/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4804 - g_loss: 0.0000e+00\n",
      "Epoch 125/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4501 - g_loss: 0.0000e+00\n",
      "Epoch 126/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4841 - g_loss: 0.0000e+00\n",
      "Epoch 127/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5455 - g_loss: 0.0000e+00\n",
      "Epoch 128/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4477 - g_loss: 0.0000e+00\n",
      "Epoch 129/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5077 - g_loss: 0.0000e+00\n",
      "Epoch 130/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5367 - g_loss: 0.0000e+00\n",
      "Epoch 131/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3861 - g_loss: 0.0000e+00\n",
      "Epoch 132/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3908 - g_loss: 0.0000e+00\n",
      "Epoch 133/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3616 - g_loss: 0.0000e+00\n",
      "Epoch 134/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3766 - g_loss: 0.0000e+00\n",
      "Epoch 135/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3898 - g_loss: 0.0000e+00\n",
      "Epoch 136/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5340 - g_loss: 0.0000e+00\n",
      "Epoch 137/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4357 - g_loss: 0.0000e+00\n",
      "Epoch 138/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4802 - g_loss: 0.0000e+00\n",
      "Epoch 139/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4236 - g_loss: 0.0000e+00\n",
      "Epoch 140/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.5645 - g_loss: 0.0000e+00\n",
      "Epoch 141/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.3968 - g_loss: 0.0000e+00\n",
      "Epoch 142/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4226 - g_loss: 0.0000e+00\n",
      "Epoch 143/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4412 - g_loss: 0.0000e+00\n",
      "Epoch 144/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - d_loss: 15.4621 - g_loss: 0.0000e+00\n",
      "Epoch 145/500\n"
     ]
    }
   ],
   "source": [
    "# Train the GAN\n",
    "train_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    './../data/images/classes',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None  # No labels for GAN training\n",
    ")\n",
    "\n",
    "hist = gan.fit(train_ds, epochs=500, callbacks=ModelMonitor(generator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "\n",
    "save_model(generator, './../models/generator.keras')  # Save generator in native Keras format\n",
    "save_model(discriminator, './../models/discriminator.keras')  # Save discriminator in native Keras format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'archive\\./../models/generator.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marchive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./../models/generator.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ana\\miniconda3\\envs\\hazmat-env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Ana\\miniconda3\\envs\\hazmat-env\\Lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Users\\Ana\\miniconda3\\envs\\hazmat-env\\Lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'archive\\./../models/generator.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "generator.load_weights(os.path.join('archive', './../models/generator.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Loss')\n",
    "plt.plot(hist.history['d_loss'], label='d_loss')\n",
    "plt.plot(hist.history['g_loss'], label='g_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Out the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.randn(4, 128)  # 4 is the batch size, 128 is the dimension of the noise vector\n",
    "\n",
    "# Generate images from the noise vectors\n",
    "generated_images = generator.predict(noise)\n",
    "\n",
    "# Inspect the generated images\n",
    "print(generated_images.shape)  # Should be (4, 200, 200, 3)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "\n",
    "# Display each generated image in a subplot\n",
    "for idx, img in enumerate(generated_images):\n",
    "    ax[idx].imshow(np.squeeze(img))\n",
    "    ax[idx].title.set_text(f\"Image {idx}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hazmat-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
